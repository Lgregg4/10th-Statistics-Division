---
title: "Lab 9"
author: "Lauren, Chris, Ryan, Katie"
date: "3/15/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(nycflights13)
library(forcats)
library(lubridate)
library(stringr)
library(dplyr)
```

# Overall Team Question
How have trends changed in political party self identification?
An answer to this question would allow politicians and law makers to understand where the country's political mind. Ideally, this knowledge would help to make policy changes in the best interest of the country's opinion.

# Conclusion
```{r}
unique(gss_cat$partyid)

data <- gss_cat %>%
  mutate(partyid = fct_recode(partyid,
                              "Republican"  = "Strong republican",
                              "Republican"  = "Not str republican",
                              "Independent" = "Ind,near rep",
                              "Independent" = "Ind,near dem",
                              "Democrat"    = "Not str democrat",
                              "Democrat"    = "Strong democrat",
                              "Other"       = "No answer",
                              "Other"       = "Don't know",
                              "Other"       = "Other party"
  )) %>%
  select(-c(marital, age, race, rincome, relig, denom, tvhours)) %>%
  count(year, partyid)  %>%
  group_by(year) %>%
  mutate(p = n / sum(n))

ggplot(data, aes(x = year, y = p, colour = fct_reorder2(partyid, year, p))) +
  geom_point() +
  geom_line() +
  labs(colour = "Party ID")
```

There are a few trends that we can observe in the political trends. The percentage of individuals who identify as a republican has been decreasing since 2004. The percentage od individuals who identify as independent seems to be, overall, increasing. 

This tells law makers and politicians that the public may be transisitioning to supporting ideas rather than parties. Perhaps, when presenting ideas, individuals shouldn't focus on the liberal or conservatice ideas, but the overall potential changes the ideas could cause.

# Individual Findings and Plots

## Lauren

### 13.4.6.2
Add the location of the origin and destination (the lat and lon) to flights.
```{r, 13.4.6.2}
airports2 <- airports %>% select(-c(alt, tz, dst, tzone, name))

flights2 <- flights %>% 
  left_join(airports2, by = c("dest" = "faa")) %>%
  rename(dest_lon = lon) %>%
  rename(dest_lat = lat)

flights2 <- flights2 %>%
  left_join(airports2, by = c("origin" = "faa" )) %>%
  rename(origin_lon = lon) %>%
  rename(origin_lat = lat)

head(flights2)
```

### 14.2.5.3
#### Use str_length() and str_sub() to extract the middle character from a string. What will you do if the string has an even number of characters?

```{r, 14.2.5.3}
string <- "Hi, my name is Lauren."
len <- str_length(string)
mid_char <- str_sub(string, len/2, len/2)
mid_char
```

The above code works with an uneven number of characters. str_sub() automatically rounds down. If I change the string to be "Hi, my name is Lauren!!", the middle character will still be 'e' even though the length of the string is 23.

### 15.5.1.1
#### How have the proportions of people identifying as Democrat, Republican, and Independent changed over time?

```{r, 15.5.1.1}
unique(gss_cat$partyid)

data <- gss_cat %>%
  mutate(partyid = fct_recode(partyid,
                              "Republican"  = "Strong republican",
                              "Republican"  = "Not str republican",
                              "Independent" = "Ind,near rep",
                              "Independent" = "Ind,near dem",
                              "Democrat"    = "Not str democrat",
                              "Democrat"    = "Strong democrat",
                              "Other"       = "No answer",
                              "Other"       = "Don't know",
                              "Other"       = "Other party"
  )) %>%
  select(-c(marital, age, race, rincome, relig, denom, tvhours)) %>%
  count(year, partyid)  %>%
  group_by(year) %>%
  mutate(p = n / sum(n))

ggplot(data, aes(x = year, y = p, colour = fct_reorder2(partyid, year, p))) +
  geom_point() +
  geom_line() +
  labs(colour = "Party ID")
```

There has not been any drastic changes in the proportions of how people identify their political party. We have seen a consistent descrease in republican identification 2004.

### 16.2.4.3
#### Use the appropriate lubridate function to parse each of the following dates:
```{r, 16.2.4.3}
d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
```
```{r, 16.4.3.d1}
mdy(d1)
```
```{r, 16.4.3.d2}
ymd(d2)
```
```{r, 16.4.3.d3}
dmy(d3)
```
```{r, 16.4.3.d4}
mdy(d4)
```
```{r, 16.4.3.d5}
mdy(d5)
```


## Chris
### 13.5.1
#### 2.Filter flights to only show flights with planes that have flown at least 100 flights.
```{r}
G100 <- filter(flights) %>%group_by(tailnum) %>%count() %>%filter(n > 100)

flights %>%
  semi_join(G100, by = "tailnum")
```


### 14.4.5 
#### 1.Replace all forward slashes in a string with backslashes.
```{r}
str = "iamgood/prettygood/\\/reallygood"
writeLines(str)
writeLines(str_replace_all(str, "/", "\\\\"))
```


### 15.5.1 Exercises
#### 2.How could you collapse rincome into a small set of categories?
```{r}
gss_cat %>%
  mutate(rincome =
           fct_collapse(
             rincome,
             `Uknow` = c("No answer", "Don't know", "Refused", "Not applicable"),
             `Lt $10000` = c("Lt $1000", str_c("$", c("1000", "3000", "4000","5000", "6000", "7000", "8000"),
                                              " to ", c("2999", "3999", "4999","5999", "6999", "7999", "9999"))),
             `$10000 to 20000` = str_c("$", c( "10000", "15000"),
                                      " - ", c("14999", "19999"))
              
           )) %>%
  ggplot(aes(x = rincome)) +
  geom_bar() 
```

### 16.3.4 Exercises
#### 5.On what day of the week should you leave if you want to minimise the chance of a delay???
```{r}
make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}

flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time"))
```


```{r}
flights_dt %>%
  mutate(leave = wday(sched_dep_time)) %>%
  group_by(leave) %>%
  summarise(dep_delay = mean(dep_delay,na.rm=TRUE),
            arr_delay = mean(arr_delay, na.rm = TRUE))
```

## Ryan
```{r}


#Problem 1: Joins
##############################################################################################################33
planes<-planes

planes<-planes%>%mutate(age=2013-year)%>%filter(!is.na(year) & !is.na(tailnum))

flights<-flights%>%filter(!is.na(arr_delay))

planes_joined <- left_join(flights,planes,by = "tailnum")

planes_joined<-planes_joined%>%select(c("tailnum","arr_delay","age"))%>%filter(!is.na(age))%>%group_by(tailnum)%>%summarise(med = median(arr_delay))

planes_joined<-planes_joined%>%arrange(med)

planes_joined <- left_join(planes_joined,planes,by = "tailnum")

planes_joined<-planes_joined%>%select("tailnum","med","age")

planes_joined<-planes_joined%>%slice(1:20)

planes_joined

cor(planes_joined$med,planes_joined$age)

```

In the first problem, I found the ages of the planes with the top 20 median delay times. Surprisingly, the ages of the most delayed planes do not exclusivley have the highest ages in the dataset. In fact, cor(med,age) returns a rather weak negative correlation (-.15).



```{r}

#Problem 2: Strings
##############################################################################################################33
#does u always come after q?
library(stringr)

words<-words

allq = sum(str_detect(words,"q."))
qu = sum(str_detect(words,"qu"))

allq
qu
```

It seems that 'u' does always come after q!


```{r}

#Problem 3:factors
##############################################################################################################33
#What is the most common relig in this survey
#What is the most common party ID

library(forcats)
survey<-gss_cat

partyID<-survey%>%count(partyid)%>%arrange(-n)
religion<-survey%>%count(relig)%>%arrange(-n)


partyID
religion

```


Here, the party identifications and religions are arranged in descending order. This shows the most to least common party identifications and religions.

```{r}

#Problem 4: Date & Time
###############################################################################################################
#How does the distribution of flight within a day times change over the course of the year?

library(lubridate)

library(reshape2)

make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
} #This function is from the book. It is used because the departure/arrival times are given in an odd form
#This converts them to hours, minutes (time%/%100 gives the arr_time or dep_time divided by 100, ignore remainder,time%%100 is the remainder)

flights_dt <- flights %>% filter(!is.na(dep_time) & !is.na(arr_time)) %>% mutate(dep_time = make_datetime_100(year, month, day, dep_time))
#using that function to alter the dep_time variable


flights_dt %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 86400)
#this gives a more general sense of how the distribution of flight times changes through the year

flights_dt<-flights_dt%>%mutate(yearday = yday(dep_time))

flights_dt%>%ggplot(aes(x=yearday))+geom_bar()
#this shows the variation in flights per day throughout the year

flights_dt<-flights_dt%>%select("dep_time","yearday")

flights_melt<-melt(flights_dt,id.vars = "dep_time")

flights_dt%>%filter(yearday == 100)%>%
  ggplot(aes(dep_time))+
  geom_freqpoly()

```

Plotting the answer to this question proved to be rather difficult, if not impossible using just one plot. I used three here. The first shows the time distribution of departure times throughout the year. The second shows how many flights occured on each day throughout the year. The third is a sample of the distribution of departure times on the 100th day of the year.


## Katie
### 13.5.1
#### 4. Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns?
```{r}
fn<- flights%>%group_by(month,day) %>% summarise(tot_delay = sum(arr_delay + dep_delay, na.rm = TRUE))%>% mutate(two_days_delay = tot_delay + lag(tot_delay))%>% arrange(desc(two_days_delay))
wea<- weather %>%
  group_by(month, day) %>%
  summarize_at(vars(humid, precip, temp), mean, na.rm = TRUE)
relation <- left_join(fn,wea,by = c("month", "day"))%>% arrange(fn$two_days_delay)

ggplot(relation)+
  geom_smooth(mapping = aes(y = temp, x = two_days_delay),method = "lm")

```
From the plot, we can see that the chances of delay in two days have some proportional relationship.
### 14.4.2
#### 1 For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.

##### 1)Find all words that start or end with x.

##### 2)Find all words that start with a vowel and end with a consonant.

##### 3)Are there any words that contain at least one of each different vowel?

```{r}
#regular expression
start_x <-str_view(words,"^x")
end_x <- str_view(words,"x$")
start_vowels <- str_view(words,"^[aeiou]")
end_consant <- str_view(words,"[^aeiou]$")
atleat <- str_view(words,"aeiou",match = TRUE)
atleat
#mutiple calls
words[str_detect(words,"^x|x$")]
str_subset(words,"^[aeiou].*[^aeiou]$")
words[str_detect(words, "a") &
        str_detect(words, "e") &
        str_detect(words, "i") &
        str_detect(words, "o") &
        str_detect(words, "u")]
```

### 15.4.1
#### 1. There are some suspiciously high numbers in tvhours. Is the mean a good summary?
```{r}
summary(gss_cat[["tvhours"]])
gss_cat %>%
  filter(!is.na(tvhours)) %>%
  ggplot(aes(x = tvhours)) +
  geom_histogram(binwidth = 1)
```
To see whether it is a good summary, it depends on what you are using for. The mean values
show most numbers of preference people have.

### 16.4.5
#### 4. Write a function that given your birthday (as a date), returns how old you are in years.
```{r}
library(lubridate)
ddays(1)
years(1)
dyears(1)/ddays(1)
next_year <- today() + years(1)
today() %--% next_year
(today() %--% next_year) / ddays(1)
(ymd("1998-03-14") %--% today())%/%dyears(1)
age<- function(birthday){
  (birthday%--% today()) %/% years(1)
}
age(ymd("1998-02-10"))
```


# Who Did What

## Lauren
I completed the assignment for this lab by solving four exercise problems: 13.4.6.2, 14.2.5.3, 15.5.1.1, and 16.2.4.3. For chapter 13, I implemented left_join() and rename(). Solving chapter 14's problem, I used str_length() and str_sub() as specified by the question. I used the most functions solving 15.5.1.1; mutate(), select(), count(), group_by(), and ggplot(). Finally, I completed 16.2.4.3 by simply using three different lubridate functions: mdy(d1), ymd(), and dmy().

Additionally, I wrote up our team question and conclusion because it was a question I had answered in my individual section.

## Katie
For this lab, I solved questions 13.5.1.4, 14.4.2.1, 15.4.1.1, 16.4.5.4. In chapter 13, I used group_by(), summaries(), and mutate() to find the total hours in two days delay. And in group_by the same key and summaries at the ??????weather variables?????? in weather, then combined them up by using left_join(). The final plot showed there are proportional relationship between temperature and the delay. For chapter 14, I used two ways to find the specific words in words collection, one is by using regular expressions, and another is by using multiple calls. To show whether mean is a good summary for ??????tvhour??????, I did ggplot and histogram. The plot indicates that the higher mean values occur in early range of tvhours. It seems like most people??????s preference lie in this range. For the date and time function, I did the interval to find the interval between year that you was born and today.

## Chris.
for this lab, I did some simple question from the books,for the second one, I did the string question just change forward slashes to backslashes, and I need to double it. then for the factor question,I decided to change rincome to four group or set, all the uknow rincome, lt than 10000, from 10000 to 20000 and large than 20000 I dont change, and use a geom_bar to show the relation. so the graph would become some simple column and into a small range. for last one I used some tb function from the books shown before, and from the tb that we got, it is very easily to say tha if we want to leave we should choose sunday.

